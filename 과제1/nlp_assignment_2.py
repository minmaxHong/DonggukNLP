# -*- coding: utf-8 -*-
"""NLP Assignment 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nUucfqoS1EoyVkEd1cHIEx-gIuX9v5TQ
"""

import pandas as pd
import string
from collections import defaultdict
from sklearn.model_selection import train_test_split
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))

# from google.colab import drive
# drive.mount('/content/drive')

# !pip install nltk==3.5
# !pip install numpy matplotlib

# Load Data
# 0 : Neg, 1 : Pos
train_df = pd.read_csv('/content/drive/MyDrive/자연어처리개론/train.tsv', sep = "\t")
test_df = pd.read_csv('/content/drive/MyDrive/자연어처리개론/dev.tsv', sep = "\t")

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def preprocess_sentence(sentence):
    translator = str.maketrans('', '', string.punctuation)
    sentence = sentence.translate(translator)

    sentence = sentence.lower()

    tokens = word_tokenize(sentence)

    tokens = [word for word in tokens if word not in stop_words]

    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return lemmatized_tokens

train_df['Tokenized_Sentence'] = train_df['sentence'].apply(preprocess_sentence)
test_df['Tokenized_Sentence'] = test_df['sentence'].apply(preprocess_sentence)

train_df.head()

train_df.info()

test_df.head()

# Prior from training
def prior(df, sentiment = True):
  label_length = len(df['label'])
  # Positive
  if sentiment:
    return len(df[df['label'] == 1]) / label_length
  # Negative
  else:
    return len(df[df['label'] == 0]) / label_length

# Clear words from test/val that are not in the train
def filter_sentence(tokensized_sentence, train_df_words):
  return [word for word in tokensized_sentence if word in train_df_words]

def clear_words_test(train_df, test_df):
  train_df_words = list(set(word_tokenize(train_df['sentence'].to_string(index = False))))
  test_df['Tokenized_Sentence'] = test_df['Tokenized_Sentence'].apply(lambda x: filter_sentence(x, train_df_words))
  return test_df


test_df = clear_words_test(train_df, test_df)

test_df.head()

# Likelihoods from training

# size of Words |V|
def size_of_words(train_df):
  return len(set(word_tokenize(train_df['sentence'].to_string(index = False))))

norm_v = size_of_words(train_df)
print(f"Size of Words: {norm_v}")

# count(w, c)
def count_w_c(train_df):
  Neg_sentence = ''
  Pos_sentence = ''
  for sentence, label in zip(train_df['sentence'], train_df['label']):
    if label == 0:
      Neg_sentence += sentence
      Neg_sentence += ' '
    else:
      Pos_sentence += sentence
      Pos_sentence += ' '

  return len(word_tokenize(Neg_sentence)), len(word_tokenize(Pos_sentence))

count_denominator_neg, count_denominator_pos = count_w_c(train_df)
print(f"Count Denominator Neg: {count_denominator_neg}")
print(f"Count Denominator Pos: {count_denominator_pos}")

# count(w_i, c)
def count_w_i_c(train_df, test_df):
  for word_token in test_df['Tokenized_Sentence'][0]:
    print(word_token)

def cal_likelihood(train_df, test_df, norm_v, count_denominator_neg, count_denominator_pos):
    word_count_neg = defaultdict(int)
    word_count_pos = defaultdict(int)

    for _, row in train_df.iterrows():
        label = row['label']
        tokenized_sentence = row['Tokenized_Sentence']

        if label == 0:
            for token in tokenized_sentence:
                word_count_neg[token] += 1
        else:
            for token in tokenized_sentence:
                word_count_pos[token] += 1

    test_df['likelihood_neg'] = 0.0
    test_df['likelihood_pos'] = 0.0
    test_df['predict'] = 0

    for idx, row in test_df.iterrows():
        tokenized_sentence = row['Tokenized_Sentence']

        likelihood_neg = 1.0
        likelihood_pos = 1.0

        for token in tokenized_sentence:
            prob_neg = (word_count_neg[token] + 1) / (count_denominator_neg + norm_v)
            likelihood_neg *= prob_neg

            prob_pos = (word_count_pos[token] + 1) / (count_denominator_pos + norm_v)
            likelihood_pos *= prob_pos

        test_df.at[idx, 'likelihood_neg'] = likelihood_neg
        test_df.at[idx, 'likelihood_pos'] = likelihood_pos

        if likelihood_pos > likelihood_neg:
            test_df.at[idx, 'predict'] = 1
        else:
            test_df.at[idx, 'predict'] = 0

    return test_df

test_df = cal_likelihood(train_df, test_df, norm_v, count_denominator_neg, count_denominator_pos)

test_df = cal_likelihood(train_df, test_df, norm_v, count_denominator_neg, count_denominator_pos)

test_df.head()

# Accuracy
labels = test_df['label']
predictions = test_df['predict']

# 정확도 계산
accuracy = accuracy_score(labels, predictions)
precision = precision_score(labels, predictions)
recall = recall_score(labels, predictions)
f1 = f1_score(labels, predictions)
conf_matrix = confusion_matrix(labels, predictions)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(classification_report(labels, predictions))