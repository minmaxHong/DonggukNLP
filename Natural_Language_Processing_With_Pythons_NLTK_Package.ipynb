{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3wJB40H6Ryn"
      },
      "source": [
        "<h1>Package Install</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuTgs16z6Ryp",
        "outputId": "3ce0cdd9-7edf-4f6d-a57b-890135269371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.10/dist-packages (3.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk==3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex6KU4sa6Ryq"
      },
      "outputs": [],
      "source": [
        "!pip install numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTqAQ7fI6Ryq"
      },
      "source": [
        "<h1> Tokenizing </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2VECGxh6Ryq"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfNZ7dZP6Ryr"
      },
      "outputs": [],
      "source": [
        "example_string = \"\"\"\n",
        "... Muad'Dib learned rapidly because his first training was in how to learn.\n",
        "... And the first lesson of all was the basic trust that he could learn.\n",
        "... It's shocking to find how many people do not believe they can learn,\n",
        "... and how many more believe learning to be difficult.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDrc_Cbm6Ryr"
      },
      "outputs": [],
      "source": [
        "print(example_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KetZMZNA6Ryr"
      },
      "outputs": [],
      "source": [
        "sent_tokenize(example_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEfEr_V16Ryr"
      },
      "outputs": [],
      "source": [
        "word_tokenize(example_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEfaN7r46Rys"
      },
      "source": [
        "<h1>Filtering Stop Words</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhgv_vZz6Rys"
      },
      "outputs": [],
      "source": [
        "# 불용어(의미가 적은 단어들)를 제거하기 위해 사용한다\n",
        "nltk.download(\"stopwords\") # 불용어 목록 포함되어있다.\n",
        "from nltk.corpus import stopwords # 불용어 접근 가능\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INwNOgYU6Rys"
      },
      "outputs": [],
      "source": [
        "worf_quote = \"Sir, I protest. I am not a merry man!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPYNs_yx6Rys"
      },
      "outputs": [],
      "source": [
        "words_in_quote = word_tokenize(worf_quote)\n",
        "words_in_quote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np8uNX_V6Rys"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpDEOpf46Ryt"
      },
      "outputs": [],
      "source": [
        "filtered_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f4R1hI-6Ryt"
      },
      "outputs": [],
      "source": [
        "# 단어 불용어 제거\n",
        "for word in words_in_quote:\n",
        "    # 현재 단어를 소문자로 변환하여 불용어 목록(stop words)에 있는지 확인(불용어에 없는 것만 확인하는 과정)\n",
        "    if word.casefold() not in stop_words: # word.casefold() : 소문자로 변환\n",
        "        filtered_list.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgBv5Ixz6Ryt"
      },
      "outputs": [],
      "source": [
        "filtered_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc-xJ5Bq6Ryt"
      },
      "source": [
        "<h1>Stemming</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7fkheeQ6Ryt"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer # stem(어간)을 추출하는 알고리즘 ex) running -> run, jumps -> jump\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i87HaoFR6Ryt"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NcOf5XP6Ryu"
      },
      "outputs": [],
      "source": [
        "string_for_stemming = \"\"\"\n",
        "... The crew of the USS Discovery discovered many discoveries.\n",
        "... Discovering is what explorers do.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwtDKdKz6Ryu"
      },
      "outputs": [],
      "source": [
        "words = word_tokenize(string_for_stemming)\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGJ_AQuP6Ryu"
      },
      "outputs": [],
      "source": [
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRzkXd2Z6Ryu"
      },
      "source": [
        "<h1>Tagging Parts of Speech</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8_5Izy46Ryu"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sagan_quote = \"\"\"\n",
        "... If you wish to make an apple pie from scratch,\n",
        "... you must first invent the universe.\"\"\"\n",
        "words_in_sagan_quote = word_tokenize(sagan_quote)\n",
        "\n",
        "words_in_sagan_quote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyh-7bVf6Ryu"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger') # 단어의 품사를 tag를 부여하는 도구\n",
        "nltk.pos_tag(words_in_sagan_quote)\n",
        "\n",
        "'''\n",
        "IN : 전치사, 접속사\n",
        "PRP : 인칭대명사\n",
        "VBP : 현재형 동사\n",
        "등등...\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "k5l1eY9g6Ryu"
      },
      "outputs": [],
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ1IgLJl6Ryv"
      },
      "source": [
        "<h1>Lemmatizing</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj9h-vwt6Ryv"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer # 단어의 형태를 원래의 사전 형태로 변환하는 과정\n",
        "nltk.download('wordnet') # 단어의 의미, 관계, 품사 정보 등을 포함하는 데이터이다.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize(\"scarves\") # 변환 예시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0ZY5Qvh6Ryv"
      },
      "outputs": [],
      "source": [
        "string_for_lemmatizing = \"The friends of DeSoto love scarves.\"\n",
        "words = word_tokenize(string_for_lemmatizing)\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-EzR0zv6Ryv"
      },
      "outputs": [],
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thvVQKDB6Ryv"
      },
      "outputs": [],
      "source": [
        "lemmatizer.lemmatize(\"worst\", pos=\"a\") # pos = 'a' --> 단어의 품사가 형용사임을 지정한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QboHIb3x6Ryv"
      },
      "source": [
        "<h1>Chunking</h1>\n",
        "<h3>Note: A phrase is a word or group of words that works as a single unit to perform a grammatical function. Noun phrases are built around a noun.</h3>\n",
        "<h5>Here are some examples:</h5>\n",
        "\n",
        "<h5>“A planet”</h5>\n",
        "<h5>“A tilting planet”</h5>\n",
        "<h5>“A swiftly tilting planet” </h5>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Chunking은 텍스트에서 문법적으로 관련된 단어 또는 구를 식별하고 그룹화하는 자연어 처리 기법입니다. 이 기법을 사용하여 문장 내에서 명사구나 동사구와 같은 구문을 찾아내어 분석하거나 처리할 수 있습니다.\n",
        "\n",
        "구는 문법적 기능을 수행하기 위해 단어 또는 단어 그룹으로 구성된 단어 묶음입니다. 명사구는 명사를 중심으로 구성된 구문을 말하며, 문장에서 주어, 목적어 등 다양한 역할을 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOh2reyT6Ryw"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "lotr_quote = \"It's a dangerous business, Frodo, going out your door.\"\n",
        "words_in_lotr_quote = word_tokenize(lotr_quote)\n",
        "lotr_pos_tags = nltk.pos_tag(words_in_lotr_quote)\n",
        "lotr_pos_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FjmcCU56Ryw"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "명사구를 정의한다.\n",
        "NP : 명사구를 나타낸다.\n",
        "\n",
        "<DT>? : DT는 관사를 나타내는 tag로, ?는 태그가 선택적이라는 뜻이다.(관사가 있을 수도 없을 수도 있다.)\n",
        "<JJ>* : JJ는 형용사를 나타내는 tag로, *는 태그가 0개 이상이라는 뜻이다.(형용사가 없을 수도, 여러 개 있을 수도 있다.)\n",
        "<NN> : NN은 명사를 나타내는 tag이다. (명사구의 중심이 되는 명사를 나타낸다.)\n",
        "'''\n",
        "\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5WU-_zl6Ryw"
      },
      "outputs": [],
      "source": [
        "chunk_parser = nltk.RegexpParser(grammar) # 구문분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRWWMQXr6Ryw"
      },
      "outputs": [],
      "source": [
        "tree = chunk_parser.parse(lotr_pos_tags)\n",
        "tree.draw()\n",
        "tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttlDrBEo6Ryx"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnWgBXDK6Ryx"
      },
      "source": [
        "<h1>Chinking</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfH7fK5A6Ry1"
      },
      "outputs": [],
      "source": [
        "grammar = \"\"\"\n",
        "... Chunk: {<.*>+} #\n",
        "...        }<JJ>{\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGhyne_Q6Ry1"
      },
      "outputs": [],
      "source": [
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "tree = chunk_parser.parse(lotr_pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmtESdoB6Ry1"
      },
      "outputs": [],
      "source": [
        "tree\n",
        "#tree.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08mYA5Z76Ry2"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ZjoXau6Ry2"
      },
      "source": [
        "<h1>Named Entity Recognition(NER)</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmivosg26Ry2"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSjTGUpW6Ry2"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"maxent_ne_chunker\") # 개체명 인식을 수행하는데 필요한 데이터 포함\n",
        "nltk.download(\"words\")\n",
        "tree = nltk.ne_chunk(lotr_pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN-0PnH16Ry2"
      },
      "outputs": [],
      "source": [
        "#tree.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ivymTxI6Ry2"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etqKPtjN6Ry2"
      },
      "outputs": [],
      "source": [
        "quote = \"\"\"\n",
        "... Men like Schiaparelli watched the red planet—it is odd, by-the-bye, that\n",
        "... for countless centuries Mars has been the star of war—but failed to\n",
        "... interpret the fluctuating appearances of the markings they mapped so well.\n",
        "... All that time the Martians must have been getting ready.\n",
        "...\n",
        "... During the opposition of 1894 a great light was seen on the illuminated\n",
        "... part of the disk, first at the Lick Observatory, then by Perrotin of Nice,\n",
        "... and then by other observers. English readers heard of it first in the\n",
        "... issue of Nature dated August 2.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt1pFLc86Ry3"
      },
      "outputs": [],
      "source": [
        "def extract_ne(quote):\n",
        "...     words = word_tokenize(quote, language=\"english\")\n",
        "...     tags = nltk.pos_tag(words)\n",
        "...     tree = nltk.ne_chunk(tags, binary=True)\n",
        "...     return set(\n",
        "...         \" \".join(i[0] for i in t)\n",
        "...         for t in tree\n",
        "...         if hasattr(t, \"label\") and t.label() == \"NE\"\n",
        "...     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vk8JPAk6Ry3"
      },
      "outputs": [],
      "source": [
        "extract_ne(quote)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "URCyFb7UAuVB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}